{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import os\n",
    "#import Image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['3d','c0','c1','c10','c2','c3','c4','c5','c6','c7','c8','c9','e1','e2','ent','ex','exb','i','m','meet','mod','r','sew','sv','tech','wm1','wm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet50 model\n",
    "model_resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Unfreeze some of the layers for fine-tuning\n",
    "for name, child in model_resnet50.named_children():\n",
    "    if name in ['layer3', 'layer4']:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Modify the final layer for  len(dataset.classes) classes\n",
    "num_ftrs = model_resnet50.fc.in_features\n",
    "model_resnet50.fc = nn.Linear(num_ftrs,  len(classes))\n",
    "\n",
    "model_resnet50 = model_resnet50.to(device)\n",
    "\n",
    "# Define loss function and optimizer for ResNet50\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_resnet50.parameters()), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet101 model\n",
    "model_resnet101 = models.resnet101(pretrained=True)\n",
    "\n",
    "# Modify the final layer for  len(dataset.classes) classes\n",
    "num_ftrs = model_resnet101.fc.in_features\n",
    "model_resnet101.fc = nn.Linear(num_ftrs,  len(classes))\n",
    "\n",
    "model_resnet101 = model_resnet101.to(device)\n",
    "\n",
    "# Define loss function and optimizer for ResNet101\n",
    "optimizer = optim.Adam(model_resnet101.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model (ResNet18 in this case) and modify it\n",
    "model_resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_resnet18.fc.in_features\n",
    "model_resnet18.fc = nn.Linear(num_ftrs, len(classes))  # Adjusting for the number of classes\n",
    "model = model_resnet18.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "model_resnet101.load_state_dict(torch.load('./models/FC_Res101_simple/epoch_10.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "def predict_image(model, image_path, class_names):\n",
    "    image = preprocess_image(image_path)\n",
    "    image = image.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_class = class_names[predicted[0].item()]\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "class_names = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d\n"
     ]
    }
   ],
   "source": [
    "image = './frames/dataset/3d/.png'\n",
    "preprocess_image(image)\n",
    "p = predict_image(model_resnet101, image, class_names)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Accuracy: 0.8711\n",
      "Epoch 2/10, Validation Accuracy: 0.9389\n",
      "Epoch 3/10, Validation Accuracy: 0.9523\n",
      "Epoch 4/10, Validation Accuracy: 0.9637\n",
      "Epoch 5/10, Validation Accuracy: 0.9663\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path to your dataset\n",
    "data_dir = \"./frames/dataset/\"\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to a slightly larger size before cropping\n",
    "    transforms.RandomCrop(224),     # Randomly crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the images\n",
    "    transforms.RandomRotation(10),      # Randomly rotate the images\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Lambda(lambda img: overlay_human_silhouette(img)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "    \n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# List full path of silhouette images\n",
    "silhouette_image_paths = [os.path.join(\"./silhoutte/\", path) for path in os.listdir(\"./silhoutte/\") if path.endswith('.png')]\n",
    "\n",
    "# Load silhouette images\n",
    "silhouette_images = [Image.open(path).convert(\"RGBA\") for path in silhouette_image_paths]\n",
    "\n",
    "# Define a function to overlay human silhouette\n",
    "def overlay_human_silhouette(image):\n",
    "    silhouette = random.choice(silhouette_images)\n",
    "    silhouette = silhouette.resize((random.randint(50, 100), random.randint(100, 200)))\n",
    "\n",
    "    x, y = random.randint(0, image.width - silhouette.width), random.randint(0, image.height - silhouette.height)\n",
    "    image.paste(silhouette, (x, y), silhouette)\n",
    "    return image\n",
    "\n",
    "\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load pre-trained Vision Transformer\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=27)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    val_accuracy = validate(model, val_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
